# -------------------------
# Groq LLM Configuration
# -------------------------
model:
  provider: openai
  name: openai/gpt-oss-120b  # Options: llama-3.3-70b-versatile, llama-3.1-70b-versatile, mixtral-8x7b-32768, llama-3.1-8b-instant
  temperature: 0.89
  max_output_tokens: 4096
  top_p: 1
  request_delay: 1  # Delay between requests in seconds (increase if hitting rate limits)

paths:
  prompts_root: src/prompts
  output_root: output/gptoss